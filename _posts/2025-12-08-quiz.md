---
title: 【LLMs】大模型面试题
date: 2025-12-03 14:00:00 +0800
categories: [LLM]
tags: [transformer, attention, NLP, LLM]
description: 记录学习LLM过程中中的一些知识点，包括Transformer模型、注意力机制、NLP任务等。
mermaid: true
math: true
---

面试题：

transformer 和 llama 的 LN 的区别

- **Answear**：
  - transformer 中的 LN 是在每个子层（如 MHA、FFN）之后进行的 LayerNorm。
  - llama 中的 LN 是在每个子层之前进行的 LayerNorm。



RMSNorm、BatchNorm、LayerNorm 区别

手写RMSNorm


Relu 和 SwiGLU



BERT预训练任务、embedding


讲讲位置编码


encoder-decoder，encoder，decoder 区别


你认为最好的prompt范式是什么


端到端的大模型和多个小模型，各自的优缺点


模型训练时间如何估计



DP和DDP的区别

训练过程如何做模型监控

预训练数据处理

文本输入大模型到输出的过程

decoding策略

大模型结构有哪些变化

手撕 cross-attention

==== attention ===

讲讲对于attention的理解

Attention的计算步骤是什么


Attention 机制和传统的seq2seq模型有什么区别


self-attention 和 target-attention 区别


深度学习中attention 与全连接层的区别

=== Transformer ===

transformer 中的multi-head attention 中的每个头为什么要进行降维？

transformer在哪里做了权重共享，为什么可以做权重共享？

transformer 的点积模型做缩放的原因是什么？


=== MHA & MQA & GQA===









